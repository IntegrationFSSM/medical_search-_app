"""
Service pour la recherche de pathologies bas√©e sur les embeddings OpenAI et Claude
"""
import numpy as np
from pathlib import Path
import json
from openai import OpenAI
from django.conf import settings


class PathologySearchService:
    """Service de recherche de pathologies m√©dicales via embeddings."""
    
    def __init__(self, model='chatgpt-5.1'):
        """
        Initialiser le service avec le mod√®le sp√©cifi√©.
        
        Args:
            model: Mod√®le √† utiliser ('chatgpt-5.1', 'claude-4.5')
        """
        self.model = model
        self.embeddings_folder = settings.EMBEDDINGS_FOLDER
        
        # Initialiser le client selon le mod√®le choisi
        if model == 'chatgpt-5.1':
            self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
            self.embedding_model = settings.EMBEDDING_MODEL
        elif model == 'claude-4.5':
            try:
                from anthropic import Anthropic
                if not settings.CLAUDE_API_KEY:
                    raise ValueError(
                        "CLAUDE_API_KEY n'est pas configur√© dans le fichier .env. "
                        "Ajoutez votre cl√© API Claude dans le fichier .env : CLAUDE_API_KEY=votre_cl√©_ici"
                    )
                if len(settings.CLAUDE_API_KEY.strip()) == 0:
                    raise ValueError("CLAUDE_API_KEY est vide dans le fichier .env")
                
                # V√©rifier le format de la cl√© (doit commencer par sk-ant-)
                if not settings.CLAUDE_API_KEY.startswith('sk-ant-'):
                    print(f"‚ö†Ô∏è ATTENTION: La cl√© API Claude ne semble pas avoir le bon format (devrait commencer par 'sk-ant-')")
                
                # Configurer le client Claude avec un timeout de 25 secondes (sous la limite Heroku de 30s)
                import httpx
                self.client = Anthropic(
                    api_key=settings.CLAUDE_API_KEY,
                    timeout=httpx.Timeout(25.0, connect=5.0)  # 90s total, 10s pour la connexion
                )
                # Claude Sonnet 4.5 - mod√®le pour la g√©n√©ration de texte
                # Par d√©faut: claude-sonnet-4-5-20250929 (Claude Sonnet 4.5)
                self.claude_model = getattr(settings, 'CLAUDE_MODEL', 'claude-sonnet-4-5-20250929')
                self.embedding_model = 'claude-sonnet-4-5-20250929'  # Mod√®le Claude pour embeddings (fallback OpenAI)
                print(f"‚úÖ Client Claude initialis√© avec mod√®le: {self.claude_model}")
            except ImportError:
                raise ImportError("La biblioth√®que 'anthropic' n'est pas install√©e. Installez-la avec: pip install anthropic")
        else:
            raise ValueError(f"Mod√®le non support√©: {model}")
    
    def validate_medical_query(self, query):
        """
        Valider si une requ√™te est une description m√©dicale valide en utilisant GPT-4o.
        Note: La validation utilise toujours OpenAI (ChatGPT) pour des raisons de coh√©rence.
        
        Args:
            query: Texte de la requ√™te √† valider
            
        Returns:
            dict: {
                'is_valid': bool,
                'reason': str (si non valide)
            }
        """
        # Validation pr√©alable simple pour les termes m√©dicaux courants
        query_lower = query.lower().strip()
        medical_keywords = [
            'alcool', 'alcoolique', 'alcoolisme', 'd√©pendance', 'addiction',
            'anxieux', 'anxi√©t√©', 'anxiet√©', 'panique', 'phobie',
            'd√©pression', 'd√©pressif', 'd√©prime', 'tristesse',
            'trouble', 'sympt√¥me', 'symptome', 'pathologie', 'maladie',
            'patient', 'personne', 'homme', 'femme', 'enfant',
            'sommeil', 'insomnie', 'agressif', 'agression', 'violence',
            'psychiatrie', 'psychologique', 'mental', 'comportement',
            'hallucination', 'd√©lire', 'parano√Øa', 'paranoia',
            'bipolaire', 'schizophr√©nie', 'schizophrenie', 'autisme',
            'toc', 'obsession', 'compulsion', 'trauma', 'stress',
            'suicide', 'suicidaire', 'automutilation', 'mutilation'
        ]
        
        # Si la requ√™te contient un mot-cl√© m√©dical, accepter directement
        if any(keyword in query_lower for keyword in medical_keywords):
            print(f"‚úÖ Validation pr√©alable: requ√™te accept√©e (contient mot-cl√© m√©dical)")
            return {
                'is_valid': True,
                'reason': None
            }
        
        # Toujours utiliser OpenAI pour la validation, ind√©pendamment du mod√®le d'embedding
        validation_client = OpenAI(api_key=settings.OPENAI_API_KEY)
        
        try:
            prompt = f"""Tu es un validateur m√©dical EXPERT. Analyse la requ√™te suivante et d√©termine si elle contient un r√©el contenu m√©dical.

Requ√™te: "{query}"

R√àGLE PRINCIPALE: SOIS TR√àS PERMISSIF ! Accepte TOUTE description qui mentionne un probl√®me de sant√©, un comportement, un sympt√¥me ou une condition m√©dicale, m√™me de mani√®re simple ou informelle.

ACCEPTE (is_valid = true) si la requ√™te:
- Mentionne des sympt√¥mes, troubles, comportements ou conditions m√©dicales (m√™me un seul mot)
- D√©crit une situation clinique (m√™me tr√®s simple ou courte)
- Est li√©e √† la sant√© mentale, comportementale, ou physique
- Contient des termes m√©dicaux, psychologiques ou psychiatriques
- D√©crit un patient, une personne avec un probl√®me de sant√©
- Exemples VALIDES (accepte TOUS ces cas):
  * "personne trop alcoolique" ‚úÖ
  * "homme alcoolique" ‚úÖ
  * "alcoolique" ‚úÖ
  * "personne alcoolique" ‚úÖ
  * "enfant anxieux" ‚úÖ
  * "troubles du sommeil" ‚úÖ
  * "d√©pression" ‚úÖ
  * "patient agressif" ‚úÖ
  * "anxi√©t√©" ‚úÖ
  * "d√©pendance alcool" ‚úÖ
  * "trop alcoolique" ‚úÖ
  * Toute description contenant "alcool", "anxieux", "d√©pression", "trouble", "sympt√¥me", etc. ‚úÖ

REJETTE (is_valid = false) UNIQUEMENT si:
- Mots r√©p√©titifs sans sens: "blabla blabla", "test test test", "aaaa aaaa"
- Uniquement des symboles: ".....", "????", "!!!!"
- Mots al√©atoires sans rapport m√©dical: "voiture maison arbre"
- Texte incoh√©rent ou spam √©vident
- Cha√Æne de caract√®res al√©atoires: "asdfghjkl", "qwerty"

IMPORTANT: 
- Si la requ√™te contient UN SEUL terme m√©dical valide, ACCEPTE-la !
- Les descriptions courtes sont acceptables: "alcoolique", "anxieux", "d√©pression"
- Les descriptions informelles sont acceptables: "personne trop alcoolique", "trop anxieux"
- En cas de doute, ACCEPTE plut√¥t que de rejeter

R√©ponds UNIQUEMENT par un JSON valide:
{{
    "is_valid": true/false,
    "reason": "Explication courte si non valide (sinon null)"
}}"""

            response = validation_client.chat.completions.create(
                model="gpt-5",
                messages=[
                    {"role": "system", "content": "Tu es un validateur m√©dical expert. R√©ponds uniquement en JSON."},
                    {"role": "user", "content": prompt}
                ],
                max_completion_tokens=200
            )
            
            result_text = response.choices[0].message.content.strip()
            print(f"üîç Validation GPT-4o response: {result_text}")
            
            # Extraire le JSON si le texte contient du texte avant/apr√®s
            # json already imported at module level (line 6)
            import re
            
            # Essayer de trouver un JSON dans le texte
            json_match = re.search(r'\{[^}]*"is_valid"[^}]*\}', result_text)
            if json_match:
                result_text = json_match.group(0)
            
            result = json.loads(result_text)
            
            is_valid = result.get('is_valid', False)
            reason = result.get('reason', 'Requ√™te invalide')
            
            print(f"‚úÖ Validation result: is_valid={is_valid}, reason={reason}")
            
            return {
                'is_valid': is_valid,
                'reason': reason
            }
            
        except json.JSONDecodeError as e:
            print(f"‚ùå Erreur JSON parsing: {e}")
            print(f"‚ùå Response text: {result_text}")
            # En cas d'erreur de parsing, consid√©rer comme invalide par s√©curit√©
            return {
                'is_valid': False,
                'reason': 'Erreur de validation - veuillez r√©essayer'
            }
        except Exception as e:
            print(f"‚ùå Erreur validation GPT: {e}")
            # En cas d'erreur API, consid√©rer comme invalide par s√©curit√©
            return {
                'is_valid': False,
                'reason': 'Service de validation temporairement indisponible'
            }
    
    def get_embedding(self, text):
        """
        Obtenir l'embedding d'un texte via l'API du mod√®le s√©lectionn√©.
        
        Args:
            text: Texte √† convertir en embedding
            
        Returns:
            np.array: Vecteur d'embedding
        """
        text = text.replace("\n", " ")
        
        if self.model == 'chatgpt-5.1':
            # OpenAI / ChatGPT
            response = self.client.embeddings.create(
                input=[text], 
                model=self.embedding_model
            )
            return np.array(response.data[0].embedding)
        
        elif self.model == 'claude-4.5':
            # IMPORTANT: Anthropic Claude ne supporte pas actuellement d'API d'embeddings directe
            # Pour les embeddings, on utilise OpenAI (fallback)
            # Mais Claude peut √™tre utilis√© directement pour la g√©n√©ration de texte (generate_ai_diagnosis)
            try:
                # Utiliser OpenAI pour les embeddings m√™me si le mod√®le choisi est Claude
                # (Claude est utilis√© uniquement pour la g√©n√©ration de texte)
                openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)
                response = openai_client.embeddings.create(
                    input=[text], 
                    model=settings.EMBEDDING_MODEL
                )
                return np.array(response.data[0].embedding)
            except Exception as e:
                raise RuntimeError(
                    f"Erreur lors de la g√©n√©ration de l'embedding (fallback OpenAI): {str(e)}. "
                    f"Claude ne supporte pas les embeddings, donc OpenAI est utilis√© pour cette partie."
                )
            
        else:
            raise ValueError(f"Mod√®le non support√© pour les embeddings: {self.model}")
    
    def find_best_match(self, query, top_k=5, aggregation='max', model=None):
        # Note: param√®tre 'model' conserv√© pour compatibilit√© mais non utilis√©
        # (le mod√®le est d√©j√† d√©fini dans __init__)
        """
        Trouver les meilleurs fichiers correspondant √† une requ√™te.
        
        Args:
            query: Requ√™te texte
            top_k: Nombre de r√©sultats √† retourner
            aggregation: M√©thode d'agr√©gation ('max', 'mean', 'weighted_mean')
        
        Returns:
            Liste des meilleurs r√©sultats avec scores de similarit√©
        """
        import os
        folder_path = Path(self.embeddings_folder)
        
        # Debug: afficher les informations
        print(f"üîç DEBUG: embeddings_folder configur√© = {self.embeddings_folder}")
        print(f"üîç DEBUG: folder_path = {folder_path}")
        print(f"üîç DEBUG: folder_path absolu = {folder_path.absolute()}")
        print(f"üîç DEBUG: folder_path existe? = {folder_path.exists()}")
        print(f"üîç DEBUG: r√©pertoire courant = {os.getcwd()}")
        
        # Lister le contenu du r√©pertoire parent
        try:
            parent = folder_path.parent
            print(f"üîç DEBUG: contenu de {parent}:")
            for item in os.listdir(parent):
                print(f"  - {item}")
        except Exception as e:
            print(f"‚ùå DEBUG: Erreur lors du listage: {e}")
        
        if not folder_path.exists():
            return {
                'success': False,
                'error': f"Le dossier d'embeddings n'existe pas: {self.embeddings_folder} (chemin absolu: {folder_path.absolute()})",
                'results': []
            }
        
        # Rechercher les fichiers .npy
        npy_files = list(folder_path.rglob("*.npy"))
        
        if len(npy_files) == 0:
            return {
                'success': False,
                'error': "Aucun fichier d'embedding trouv√© (.npy)",
                'results': []
            }
        
        # Obtenir l'embedding de la requ√™te
        query_embedding = self.get_embedding(query)
        query_dimension = len(query_embedding)
        
        # V√©rifier la dimension des embeddings stock√©s (prendre le premier fichier comme r√©f√©rence)
        stored_dimension = None
        if len(npy_files) > 0:
            sample_embeddings = np.load(npy_files[0])
            if len(sample_embeddings) > 0:
                stored_dimension = len(sample_embeddings[0])
        
        # Si les dimensions ne correspondent pas, utiliser OpenAI en fallback
        if stored_dimension and query_dimension != stored_dimension:
            print(f"‚ö†Ô∏è Dimension incompatible: requ√™te={query_dimension}, stock√©={stored_dimension}")
            print(f"‚ö†Ô∏è Utilisation d'OpenAI en fallback pour les embeddings (mod√®le s√©lectionn√©: {self.model})")
            
            # Utiliser OpenAI pour les embeddings m√™me si un autre mod√®le est s√©lectionn√©
            openai_client = OpenAI(api_key=settings.OPENAI_API_KEY)
            response = openai_client.embeddings.create(
                input=[query], 
                model=settings.EMBEDDING_MODEL
            )
            query_embedding = np.array(response.data[0].embedding)
            query_dimension = len(query_embedding)
            print(f"‚úÖ Embedding OpenAI g√©n√©r√© avec dimension: {query_dimension}")
        
        # Rechercher dans tous les fichiers
        file_results = {}
        
        for emb_file in npy_files:
            # Charger les embeddings
            embeddings = np.load(emb_file)
            
            # V√©rifier que la dimension correspond toujours
            if len(embeddings) > 0 and len(embeddings[0]) != query_dimension:
                print(f"‚ö†Ô∏è Fichier {emb_file} ignor√©: dimension {len(embeddings[0])} != {query_dimension}")
                continue
            
            # Charger les m√©tadonn√©es
            metadata_file = str(Path(emb_file).with_suffix('.json'))
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
            except:
                continue
            
            # Calculer la similarit√© cosinus pour chaque chunk
            chunk_similarities = []
            best_chunk_id = 0
            best_chunk_text = ""
            best_similarity = 0
            
            for i, chunk_emb in enumerate(embeddings):
                # V√©rification suppl√©mentaire de la dimension
                if len(chunk_emb) != query_dimension:
                    continue
                    
                similarity = np.dot(query_embedding, chunk_emb) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_emb)
                )
                chunk_similarities.append(similarity)
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_chunk_id = i
                    best_chunk_text = metadata['chunks'][i].get('text_preview', '')
            
            # Agr√©ger les scores par fichier
            if aggregation == 'max':
                file_score = max(chunk_similarities)
            elif aggregation == 'mean':
                file_score = np.mean(chunk_similarities)
            elif aggregation == 'weighted_mean':
                weights = np.array([1.0 / (i + 1) for i in range(len(chunk_similarities))])
                weights = weights / weights.sum()
                file_score = np.sum(np.array(chunk_similarities) * weights)
            else:
                file_score = max(chunk_similarities)
            
            file_results[str(emb_file)] = {
                'file': metadata['source_file'],
                'file_name': Path(metadata['source_file']).name,
                'location': metadata['hierarchy'].get('location', 'N/A'),
                'similarity': float(file_score),
                'num_chunks': len(embeddings),
                'best_chunk_id': best_chunk_id,
                'best_chunk_text': best_chunk_text,
                'all_chunk_scores': [float(s) for s in chunk_similarities],
                'html_page': metadata.get('html_page', '')  # Ajouter le chemin HTML
            }
        
        # Trier par similarit√©
        results = sorted(
            file_results.values(), 
            key=lambda x: x['similarity'], 
            reverse=True
        )[:top_k]
        
        # V√©rifier la qualit√© des r√©sultats - seuil minimum de 60% (plus strict)
        if not results or results[0]['similarity'] < 0.6:
            return {
                'success': False,
                'error': 'Aucune correspondance trouv√©e. Veuillez v√©rifier que votre description est compl√®te et pr√©cise.',
                'error_type': 'low_similarity',
                'best_score': results[0]['similarity'] * 100 if results else 0,
                'results': []
            }
        
        # Ajouter des informations diagnostiques
        diagnostic_info = self._generate_diagnostic_info(results)
        
        return {
            'success': True,
            'results': results,
            'diagnostic_info': diagnostic_info,
            'total_files_searched': len(file_results)
        }
    
    def _generate_diagnostic_info(self, results):
        """G√©n√©rer des informations diagnostiques bas√©es sur les r√©sultats."""
        if not results:
            return {
                'suspected_pathology': None,
                'confidence': 0,
                'confidence_level': 'none'
            }
        
        top_match = results[0]
        similarity_percent = top_match['similarity'] * 100
        
        pathology = top_match['file_name'].replace('.txt', '').replace('_', ' ')
        
        if similarity_percent >= 75:
            confidence_level = 'high'
            message = "Forte correspondance diagnostique"
        elif similarity_percent >= 60:
            confidence_level = 'moderate'
            message = "Correspondance mod√©r√©e - Envisager un diagnostic diff√©rentiel"
        else:
            confidence_level = 'low'
            message = "Faible confiance - Informations cliniques suppl√©mentaires n√©cessaires"
        
        return {
            'suspected_pathology': pathology,
            'confidence': similarity_percent,
            'confidence_level': confidence_level,
            'message': message
        }
    
    def generate_ai_diagnosis(self, pathology_name, form_data, similarity_score, medical_text="", historical_symptoms=None):
        """
        G√©n√©rer uniquement le plan de traitement avec OpenAI ou Claude
        bas√© sur les donn√©es du formulaire, le texte m√©dical et l'historique.
        
        Args:
            pathology_name: Nom de la pathologie valid√©e
            historical_symptoms: Liste des sympt√¥mes/crit√®res des consultations pr√©c√©dentes (optionnel)
            form_data: Donn√©es du formulaire (dict avec tous les crit√®res coch√©s)
            similarity_score: Score de similarit√© de la recherche
            medical_text: Texte m√©dical extrait du fichier source (documentation DSM-5-TR)
        
        Returns:
            dict: Plan de traitement g√©n√©r√© par l'IA
        """
        try:
            # Message syst√®me pour le PLAN DE TRAITEMENT
            system_message_treatment = (
                "Vous √™tes un psychiatre clinicien expert du DSM-5-TR. "
                "Vous r√©digez un plan de traitement d√©taill√© et structur√© en fran√ßais pour le patient. "
                "Incluez : activit√©s th√©rapeutiques (suivi th√©rapeutique), prise en charge m√©dicale si n√©cessaire, "
                "recommandations psychoth√©rapeutiques, et suivi √† long terme. "
                "Basez-vous sur les recommandations officielles (HAS, OMS, soci√©t√©s savantes). "
                "Si une information manque pour √©tablir un plan s√ªr, indiquez-le clairement."
            )
            
            # üÜï Construire le prompt pour le plan de traitement directement
            treatment_prompt = self._build_treatment_prompt(
                pathology_name, 
                form_data, 
                "",  # Pas de diagnostic text, on g√©n√®re directement le plan
                medical_text, 
                historical_symptoms
            )
            
            # üÜï G√âN√âRER UNIQUEMENT LE PLAN DE TRAITEMENT
            print("üîÑ G√©n√©ration du plan de traitement...")
            
            # Appeler l'API selon le mod√®le s√©lectionn√©
            if self.model == 'chatgpt-5.1':
                # OpenAI / ChatGPT
                response = self.client.chat.completions.create(
                    model="gpt-5",
                    messages=[
                        {
                            "role": "system",
                            "content": system_message_treatment
                        },
                        {
                            "role": "user",
                            "content": treatment_prompt
                        }
                    ],
                    max_completion_tokens=1200  # RÔøΩduit pour des rÔøΩponses plus rapides (Heroku timeout 30s)
                )
                # Debug: afficher la r√©ponse compl√®te
                print(f"üîç DEBUG ChatGPT response type: {type(response)}")
                print(f"üîç DEBUG ChatGPT response.choices: {response.choices if hasattr(response, 'choices') else 'N/A'}")
                if hasattr(response, 'choices') and response.choices:
                    print(f"üîç DEBUG ChatGPT response.choices[0]: {response.choices[0]}")
                    if hasattr(response.choices[0], 'message'):
                        print(f"üîç DEBUG ChatGPT response.choices[0].message: {response.choices[0].message}")
                        if hasattr(response.choices[0].message, 'content'):
                            print(f"üîç DEBUG ChatGPT content type: {type(response.choices[0].message.content)}")
                            print(f"üîç DEBUG ChatGPT content length: {len(response.choices[0].message.content) if response.choices[0].message.content else 0}")
                
                # Extraire le contenu de la r√©ponse
                if response.choices and len(response.choices) > 0:
                    treatment_plan_text = response.choices[0].message.content
                    if not treatment_plan_text:
                        treatment_plan_text = ""
                        print(f"‚ö†Ô∏è R√©ponse ChatGPT vide - response.choices[0].message.content est None ou vide")
                        # Afficher plus de d√©tails pour le d√©bogage
                        print(f"üîç DEBUG - response.choices[0].message: {response.choices[0].message}")
                        print(f"üîç DEBUG - response.choices[0].finish_reason: {response.choices[0].finish_reason if hasattr(response.choices[0], 'finish_reason') else 'N/A'}")
                else:
                    treatment_plan_text = ""
                    print(f"‚ö†Ô∏è R√©ponse ChatGPT sans choix - response.choices est vide")
                    print(f"üîç DEBUG - response complet: {response}")
                
            elif self.model == 'claude-4.5':
                # Claude Sonnet 4.5 - utilisation directe (sans embeddings)
                try:
                    # V√©rifier que la cl√© API est configur√©e
                    if not settings.CLAUDE_API_KEY:
                        raise ValueError("CLAUDE_API_KEY n'est pas configur√© dans le fichier .env")
                    
                    print(f"üîç Appel API Claude avec mod√®le: {self.claude_model}")
                    print(f"üîç Cl√© API pr√©sente: {'Oui' if settings.CLAUDE_API_KEY else 'Non'}")
                    
                    response = self.client.messages.create(
                        model=self.claude_model,  # Claude Sonnet 4.5
                        max_tokens=1200,  # RÔøΩduit pour des rÔøΩponses plus rapides (Heroku timeout 30s)
                        temperature=0.4,
                        system=system_message_treatment,
                        messages=[
                            {
                                "role": "user",
                                "content": treatment_prompt
                            }
                        ]
                    )
                    
                    print(f"‚úÖ R√©ponse Claude re√ßue: type={type(response)}")
                    print(f"‚úÖ Response.content: {response.content if hasattr(response, 'content') else 'N/A'}")
                    
                    # Claude retourne response.content qui est une liste de TextBlock
                    # Le premier bloc contient le texte (format: TextBlock avec attribut .text)
                    if hasattr(response, 'content') and response.content and len(response.content) > 0:
                        first_content = response.content[0]
                        
                        # Claude SDK retourne un objet TextBlock avec attribut .text
                        if hasattr(first_content, 'text'):
                            treatment_plan_text = first_content.text
                            print(f"‚úÖ Plan de traitement extrait: {len(treatment_plan_text)} caract√®res")
                        else:
                            # Fallback si format diff√©rent
                            treatment_plan_text = str(first_content)
                            print(f"‚ö†Ô∏è Format inattendu, conversion en string: {len(treatment_plan_text)} caract√®res")
                    else:
                        error_msg = f"R√©ponse Claude vide - response.content: {getattr(response, 'content', 'N/A')}"
                        print(f"‚ùå {error_msg}")
                        raise ValueError(error_msg)
                    
                    if not treatment_plan_text or len(treatment_plan_text.strip()) == 0:
                        raise ValueError("Le plan de traitement g√©n√©r√© par Claude est vide")
                    
                except Exception as claude_error:
                    # Afficher l'erreur d√©taill√©e pour le d√©bogage
                    import traceback
                    error_detail = traceback.format_exc()
                    error_msg = f"Erreur API Claude: {str(claude_error)}"
                    print(f"‚ùå {error_msg}")
                    print(f"‚ùå Mod√®le utilis√©: {self.claude_model}")
                    print(f"‚ùå Cl√© API configur√©e: {'Oui' if settings.CLAUDE_API_KEY else 'Non'}")
                    print(f"‚ùå D√©tails de l'erreur:\n{error_detail}")
                    raise RuntimeError(f"{error_msg}\n\nD√©tails: {error_detail}")
                
            else:
                raise ValueError(f"Mod√®le non support√© pour la g√©n√©ration: {self.model}")
            
            print(f"‚úÖ Plan de traitement g√©n√©r√©: {len(treatment_plan_text)} caract√®res")
            
            return {
                'success': True,
                'pathology': pathology_name,
                'diagnosis': '',  # Pas de diagnostic summary
                'treatment_plan': treatment_plan_text,  # Uniquement le plan de traitement
                'confidence': similarity_score,
                'timestamp': self._get_timestamp(),
                'model_used': self.model
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'pathology': pathology_name,
                'model_used': self.model
            }
    
    def _build_diagnosis_prompt(self, pathology_name, form_data, similarity_score, medical_text="", historical_symptoms=None):
        """Construire le prompt pour OpenAI avec le texte m√©dical et l'historique du patient."""
        
        prompt = f"""√âlabore un R√âSUM√â DIAGNOSTIQUE (sans plan th√©rapeutique) pour un patient √©valu√© selon le DSM-5-TR.

Consignes obligatoires :
- Baser l'analyse UNIQUEMENT sur les crit√®res coch√©s ci-dessous et sur l'extrait m√©dical fourni.
- Ne jamais prescrire ni d√©crire un traitement m√©dicamenteux ou une posologie.
- Utiliser un ton clinique, structur√© et concis en fran√ßais.

Informations de r√©f√©rence :
‚Ä¢ Pathologie suspect√©e : {pathology_name}
‚Ä¢ Niveau de correspondance : {similarity_score:.1f}%

Extrait DSM-5-TR disponible :
{medical_text if medical_text else "Aucun extrait suppl√©mentaire. S'appuyer uniquement sur les crit√®res coch√©s."}

Crit√®res et √©l√©ments cliniques d√©clar√©s :
"""
        
        # üÜï AJOUTER L'HISTORIQUE M√âDICAL
        if historical_symptoms and len(historical_symptoms) > 0:
            prompt += f"\nüìã **ANT√âC√âDENTS M√âDICAUX DU PATIENT ({len(historical_symptoms)} sympt√¥mes enregistr√©s):**\n"
            prompt += "Le patient pr√©sente √©galement les ant√©c√©dents cliniques suivants, issus de consultations pr√©c√©dentes:\n"
            for i, symptom in enumerate(historical_symptoms[:15], 1):  # Limiter √† 15 pour ne pas surcharger
                prompt += f"  ‚Ä¢ {symptom}\n"
            if len(historical_symptoms) > 15:
                prompt += f"  ‚Ä¢ ... et {len(historical_symptoms) - 15} autres sympt√¥mes enregistr√©s\n"
            prompt += "\n**‚ö†Ô∏è IMPORTANT : Int√©grer ces ant√©c√©dents dans l'analyse diagnostique.**\n\n"
        
        prompt += """
"""
        
        # Ajouter les donn√©es du formulaire
        if isinstance(form_data, dict):
            for key, value in form_data.items():
                if value:  # Si la valeur n'est pas vide
                    if isinstance(value, list):
                        prompt += f"\n**{key}:**\n"
                        for item in value:
                            prompt += f"  ‚úì {item}\n"
                    else:
                        prompt += f"\n**{key}:** {value}\n"
        
        prompt += """

Structure attendue (respecter EXACTEMENT ces titres) :

## 1. Synth√®se clinique
- 2 √† 3 phrases r√©sumant la pr√©sentation clinique et le niveau de confiance.

## 2. Crit√®res DSM-5 confirm√©s
- Reprendre les crit√®res coch√©s (par blocs si possible) avec le nombre total valid√©.

## 3. Diagnostic diff√©rentiel prioritaire
- 3 √† 5 hypoth√®ses maximum, chacune justifi√©e bri√®vement.

## 4. Comorbidit√©s / facteurs associ√©s
- √âl√©ments issus du formulaire ou traditionnellement li√©s √† la pathologie, avec lien clinique.

## 5. Recommandations cliniques imm√©diates
- √âtapes de suivi, examens compl√©mentaires, coordination interdisciplinaire ou psycho√©ducation.
- INTERDIT : citer des mol√©cules, dosages, ou protocoles th√©rapeutiques.
"""
        
        return prompt
    
    def _generate_treatment_plan(self, pathology_name, form_data, diagnosis_text, medical_text="", historical_symptoms=None, system_message=None):
        """
        G√©n√©rer un plan de traitement d√©taill√© pour le patient.
        
        Args:
            pathology_name: Nom de la pathologie
            form_data: Donn√©es du formulaire
            diagnosis_text: Texte du diagnostic g√©n√©r√©
            medical_text: Texte m√©dical extrait
            historical_symptoms: Historique des sympt√¥mes
            system_message: Message syst√®me pour le plan de traitement
            
        Returns:
            str: Plan de traitement g√©n√©r√©
        """
        try:
            # Construire le prompt pour le plan de traitement
            treatment_prompt = self._build_treatment_prompt(
                pathology_name, 
                form_data, 
                diagnosis_text, 
                medical_text, 
                historical_symptoms
            )
            
            # G√©n√©rer le plan de traitement avec le m√™me mod√®le
            if self.model == 'chatgpt-5.1':
                response = self.client.chat.completions.create(
                    model="gpt-5",
                    messages=[
                        {
                            "role": "system",
                            "content": system_message
                        },
                        {
                            "role": "user",
                            "content": treatment_prompt
                        }
                    ],
                    max_completion_tokens=1200  # RÔøΩduit pour des rÔøΩponses plus rapides (Heroku timeout 30s)
                )
                # Debug: afficher la r√©ponse compl√®te
                print(f"üîç DEBUG ChatGPT response type: {type(response)}")
                print(f"üîç DEBUG ChatGPT response.choices: {response.choices if hasattr(response, 'choices') else 'N/A'}")
                if hasattr(response, 'choices') and response.choices:
                    print(f"üîç DEBUG ChatGPT response.choices[0]: {response.choices[0]}")
                    if hasattr(response.choices[0], 'message'):
                        print(f"üîç DEBUG ChatGPT response.choices[0].message: {response.choices[0].message}")
                        if hasattr(response.choices[0].message, 'content'):
                            print(f"üîç DEBUG ChatGPT content type: {type(response.choices[0].message.content)}")
                            print(f"üîç DEBUG ChatGPT content length: {len(response.choices[0].message.content) if response.choices[0].message.content else 0}")
                
                # Extraire le contenu de la r√©ponse
                if response.choices and len(response.choices) > 0:
                    treatment_plan_text = response.choices[0].message.content
                    if not treatment_plan_text:
                        treatment_plan_text = ""
                        print(f"‚ö†Ô∏è R√©ponse ChatGPT vide - response.choices[0].message.content est None ou vide")
                        # Afficher plus de d√©tails pour le d√©bogage
                        print(f"üîç DEBUG - response.choices[0].message: {response.choices[0].message}")
                        print(f"üîç DEBUG - response.choices[0].finish_reason: {response.choices[0].finish_reason if hasattr(response.choices[0], 'finish_reason') else 'N/A'}")
                else:
                    treatment_plan_text = ""
                    print(f"‚ö†Ô∏è R√©ponse ChatGPT sans choix - response.choices est vide")
                    print(f"üîç DEBUG - response complet: {response}")
                
            elif self.model == 'claude-4.5':
                response = self.client.messages.create(
                    model=self.claude_model,
                    max_tokens=1200,  # RÔøΩduit pour des rÔøΩponses plus rapides (Heroku timeout 30s)
                    temperature=0.4,
                    system=system_message,
                    messages=[
                        {
                            "role": "user",
                            "content": treatment_prompt
                        }
                    ]
                )
                if hasattr(response, 'content') and response.content and len(response.content) > 0:
                    treatment_plan_text = response.content[0].text
                else:
                    raise ValueError("R√©ponse Claude vide pour le plan de traitement")
            else:
                raise ValueError(f"Mod√®le non support√© pour le plan de traitement: {self.model}")
            
            print(f"‚úÖ Plan de traitement g√©n√©r√©: {len(treatment_plan_text)} caract√®res")
            return treatment_plan_text
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la g√©n√©ration du plan de traitement: {str(e)}")
            return f"Erreur lors de la g√©n√©ration du plan de traitement: {str(e)}"
    
    def _build_treatment_prompt(self, pathology_name, form_data, diagnosis_text="", medical_text="", historical_symptoms=None):
        """
        Construire le prompt pour g√©n√©rer le plan de traitement.
        """
        prompt = f"""G√©n√®re un PLAN DE TRAITEMENT d√©taill√© et structur√© en fran√ßais pour un patient.

INFORMATIONS DU PATIENT :
‚Ä¢ Pathologie identifi√©e : {pathology_name}

TEXTE M√âDICAL DE R√âF√âRENCE :
{medical_text if medical_text else "Aucun extrait suppl√©mentaire."}

CRIT√àRES VALID√âS :
"""
        
        # Ajouter les donn√©es du formulaire
        if isinstance(form_data, dict):
            for key, value in form_data.items():
                if key != '_metadata' and value:  # Exclure les m√©tadonn√©es
                    if isinstance(value, list):
                        prompt += f"\n**{key}:**\n"
                        for item in value:
                            prompt += f"  ‚úì {item}\n"
                    else:
                        prompt += f"\n**{key}:** {value}\n"
        
        # Ajouter l'historique si disponible
        if historical_symptoms and len(historical_symptoms) > 0:
            prompt += f"\nüìã **ANT√âC√âDENTS M√âDICAUX ({len(historical_symptoms)} sympt√¥mes enregistr√©s):**\n"
            for symptom in historical_symptoms[:10]:  # Limiter √† 10
                prompt += f"  ‚Ä¢ {symptom}\n"
        
        prompt += """

STRUCTURE ATTENDUE DU PLAN DE TRAITEMENT :

## 1. Suivi Th√©rapeutique (Activit√©s Th√©rapeutiques)
- Indiquer le type de suivi recommand√© (fr√©quence, dur√©e)
- Modalit√©s de suivi (consultations, t√©l√©consultations, etc.)

## 2. Prise en Charge M√©dicale (si n√©cessaire)
- Recommandations m√©dicales g√©n√©rales
- Suivi des comorbidit√©s physiques si pr√©sentes

## 3. Interventions Psychoth√©rapeutiques
- Type de psychoth√©rapie recommand√©e
- Objectifs th√©rapeutiques
- Dur√©e et fr√©quence

## 4. Suivi √† Long Terme
- Planification du suivi sur plusieurs mois
- Points de vigilance
- Crit√®res d'am√©lioration attendus

IMPORTANT : 
- Base-toi uniquement sur les informations fournies
- Utilise un langage m√©dical professionnel
- Inclus le suivi th√©rapeutique (activit√©s th√©rapeutiques) comme demand√©
- Sois pr√©cis mais adapt√© au cas du patient
- NE PAS ajouter de phrases de conclusion, de disclaimer ou de note sur l'ajustement du plan
- Terminer directement apr√®s la section 4 sans phrase de cl√¥ture
"""
        
        return prompt
    
    def _get_timestamp(self):
        """Obtenir le timestamp actuel."""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

