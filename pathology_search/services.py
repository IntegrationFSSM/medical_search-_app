"""
Service pour la recherche de pathologies bas√©e sur les embeddings OpenAI et Claude
"""
import numpy as np
from pathlib import Path
import json
from openai import OpenAI
from django.conf import settings


class PathologySearchService:
    """Service de recherche de pathologies m√©dicales via embeddings."""
    
    def __init__(self, model='chatgpt-5.1', embedding_model_type='openai-ada'):
        """
        Initialiser le service avec le mod√®le sp√©cifi√©.
        
        Args:
            model: Mod√®le de g√©n√©ration de texte ('chatgpt-5.1', 'claude-4.5')
            embedding_model_type: Mod√®le d'embedding ('openai-ada', 'openai-3-large', 'gemini')
        """
        self.model = model
        self.embedding_model_type = embedding_model_type
        
        # D√©finir le dossier d'embeddings selon le mod√®le choisi
        if embedding_model_type == 'openai-3-large':
            self.embeddings_folder = settings.BASE_DIR / 'Embedding_OpenAI_3072'
            self.embedding_model_name = 'text-embedding-3-large'
            self.embedding_dim = 3072
        elif embedding_model_type == 'gemini':
            self.embeddings_folder = settings.BASE_DIR / 'Embedding_Gemini_3072'
            self.embedding_model_name = 'models/gemini-embedding-001' # Ou text-embedding-004 selon dispo
            self.embedding_dim = 3072
            
            # Configurer Gemini pour les embeddings si n√©cessaire
            import google.generativeai as genai
            if not settings.GEMINI_API_KEY:
                print("‚ö†Ô∏è Cl√© API Gemini manquante dans les settings")
            else:
                genai.configure(api_key=settings.GEMINI_API_KEY)
        else:
            # Par d√©faut: OpenAI ada-002
            self.embeddings_folder = settings.EMBEDDINGS_FOLDER
            self.embedding_model_name = settings.EMBEDDING_MODEL
            self.embedding_dim = 1536
            
        # Ne pas afficher les logs d'embeddings si on ne fait que g√©n√©rer (pas de recherche)
        # Les logs seront affich√©s uniquement lors de l'utilisation de find_best_match
        # print(f"üìÇ Dossier embeddings utilis√©: {self.embeddings_folder}")
        # print(f"üß† Mod√®le embedding: {self.embedding_model_type} ({self.embedding_model_name})")
        
        # Initialiser le client OpenAI (toujours n√©cessaire pour certaines fonctions ou fallback)
        self.client = OpenAI(api_key=settings.OPENAI_API_KEY)
        
        # Initialiser le client Claude si n√©cessaire
        if model == 'claude-4.5':
            try:
                from anthropic import Anthropic
                if not settings.CLAUDE_API_KEY:
                    raise ValueError("La cl√© API Claude n'est pas configur√©e dans les variables d'environnement (.env)")
                
                self.claude_client = Anthropic(api_key=settings.CLAUDE_API_KEY)
                self.claude_model = getattr(settings, 'CLAUDE_MODEL', 'claude-sonnet-4-5-20250929')
                print(f"‚úÖ Client Claude initialis√© avec mod√®le: {self.claude_model}")
            except ImportError:
                raise ImportError("La biblioth√®que 'anthropic' n'est pas install√©e. Installez-la avec: pip install anthropic")
        
        # üÜï Gemini supprim√© pour la g√©n√©ration - seulement Model 1 (ChatGPT) et Model 2 (Claude) sont disponibles

    def validate_medical_query(self, query):
        """
        Valider si une requ√™te est une description m√©dicale valide en utilisant GPT-4o.
        Note: La validation utilise toujours OpenAI (ChatGPT) pour des raisons de coh√©rence.
        
        Args:
            query: Texte de la requ√™te √† valider
            
        Returns:
            dict: {
                'is_valid': bool,
                'reason': str (si non valide)
            }
        """
        # Validation pr√©alable simple pour les termes m√©dicaux courants
        query_lower = query.lower().strip()
        medical_keywords = [
            'alcool', 'alcoolique', 'alcoolisme', 'd√©pendance', 'addiction',
            'drogue', 'cannabis', 'coca√Øne', 'h√©ro√Øne', 'opiac√©s',
            'anxi√©t√©', 'anxieux', 'peur', 'panique', 'stress', 'phobie',
            'd√©pression', 'd√©prim√©', 'triste', 'suicide', 'humeur',
            'bipolaire', 'manie', 'maniaque',
            'schizophr√©nie', 'psychose', 'hallucination', 'd√©lire',
            'trouble', 'syndrome', 'maladie', 'pathologie', 'sympt√¥me',
            'douleur', 'fatigue', 'insomnie', 'sommeil',
            'manger', 'app√©tit', 'poids', 'boulimie', 'anorexie',
            'm√©moire', 'concentration', 'attention', 'hyperactif', 'tdah',
            'toc', 'obsession', 'compulsion',
            'trauma', 'ptsd', 'stress post-traumatique',
            'personnalit√©', 'bordeline', 'limite', 'antisocial',
            'sexuel', 'sexuelle', 'libido', '√©rection', '√©jaculation',
            'enfant', 'adolescent', 'adulte', 'femme', 'homme',
            'patient', 'patiente', 'sujet', 'cas',
            'diagnostic', 'traitement', 'm√©dicament', 'th√©rapie'
        ]
        
        # Si la requ√™te est tr√®s courte et contient un mot cl√©, on accepte
        if len(query.split()) < 5 and any(keyword in query_lower for keyword in medical_keywords):
            return {'is_valid': True, 'reason': 'Terme m√©dical d√©tect√©'}

        try:
            # Utiliser un client OpenAI d√©di√© pour la validation (ind√©pendant du mod√®le choisi pour le reste)
            validation_client = OpenAI(api_key=settings.OPENAI_API_KEY)
            
            prompt = f"""Tu es un validateur m√©dical EXPERT. Analyse la requ√™te suivante et d√©termine si elle contient un r√©el contenu m√©dical.

Requ√™te: "{query}"

R√àGLE PRINCIPALE: SOIS TR√àS PERMISSIF ! Accepte TOUTE description qui mentionne un probl√®me de sant√©, un comportement, un sympt√¥me ou une condition m√©dicale, m√™me de mani√®re simple ou informelle.

ACCEPTE (is_valid = true) si la requ√™te:
- Mentionne des sympt√¥mes, troubles, comportements ou conditions m√©dicales (m√™me vagues)
- D√©crit un √©tat psychologique ou physique probl√©matique
- Raconte une histoire de patient ou un cas clinique
- Pose une question sur une maladie ou un traitement
- Contient des mots-cl√©s m√©dicaux ou psychologiques

REFUSE (is_valid = false) UNIQUEMENT si la requ√™te est:
- Totalement incoh√©rente ou vide de sens (gibberish)
- Clairement du spam ou du contenu malveillant
- Une demande de code informatique, de recette de cuisine, ou autre sujet 100% non m√©dical
- Une simple salutation sans suite ("bonjour", "salut")

R√©ponds UNIQUEMENT au format JSON:
{{
    "is_valid": true/false,
    "reason": "Explication tr√®s br√®ve (1 phrase)"
}}
"""

            response = validation_client.chat.completions.create(
                model="gpt-4o",  # Utiliser un mod√®le rapide et performant pour la validation
                messages=[
                    {"role": "system", "content": "Tu es un assistant de validation strict qui r√©pond uniquement en JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                response_format={"type": "json_object"}
            )
            
            result_text = response.choices[0].message.content
            
            # Essayer de trouver un JSON dans le texte
            import re
            json_match = re.search(r'\{[^}]*"is_valid"[^}]*\}', result_text)
            if json_match:
                result_text = json_match.group(0)
            
            result = json.loads(result_text)
            
            is_valid = result.get('is_valid', False)
            reason = result.get('reason', 'Requ√™te invalide')
            
            print(f"‚úÖ Validation result: is_valid={is_valid}, reason={reason}")
            
            return {
                'is_valid': is_valid,
                'reason': reason
            }
            
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è Erreur de d√©codage JSON lors de la validation: {e}")
            # En cas d'erreur de parsing, on est permissif
            return {'is_valid': True, 'reason': 'Validation technique √©chou√©e (fallback)'}
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la validation m√©dicale: {e}")
            # En cas d'erreur API, on est permissif pour ne pas bloquer l'utilisateur
            return {'is_valid': True, 'reason': 'Erreur de validation (fallback)'}
    
    def get_embedding(self, text):
        """
        Obtenir l'embedding d'un texte via l'API du mod√®le s√©lectionn√©.
        
        Args:
            text: Texte √† convertir en embedding
            
        Returns:
            np.array: Vecteur d'embedding
        """
        text = text.replace("\n", " ")
        
        try:
            if self.embedding_model_type == 'gemini':
                import google.generativeai as genai
                # Gemini Embedding
                result = genai.embed_content(
                    model=self.embedding_model_name,
                    content=text,
                    task_type="retrieval_query"
                )
                return np.array(result['embedding'])
                
            elif self.embedding_model_type == 'openai-3-large':
                # OpenAI text-embedding-3-large
                print(f"üîç DEBUG - G√©n√©ration embedding avec text-embedding-3-large")
                response = self.client.embeddings.create(
                    input=[text], 
                    model=self.embedding_model_name
                )
                embedding = np.array(response.data[0].embedding)
                print(f"‚úÖ Embedding g√©n√©r√© - Dimension: {len(embedding)} (attendu: {self.embedding_dim})")
                return embedding
                
            else:
                # OpenAI text-embedding-ada-002 (D√©faut)
                print(f"üîç DEBUG - G√©n√©ration embedding avec {self.embedding_model_name}")
                response = self.client.embeddings.create(
                    input=[text], 
                    model=self.embedding_model_name
                )
                embedding = np.array(response.data[0].embedding)
                print(f"‚úÖ Embedding g√©n√©r√© - Dimension: {len(embedding)} (attendu: {self.embedding_dim})")
                return embedding
                
        except Exception as e:
            print(f"‚ùå Erreur g√©n√©ration embedding ({self.embedding_model_type}): {str(e)}")
            raise
    
    def find_best_match(self, query, top_k=5, aggregation='max', model=None):
        # Note: param√®tre 'model' conserv√© pour compatibilit√© mais non utilis√©
        # (le mod√®le est d√©j√† d√©fini dans __init__)
        """
        Trouver les meilleurs fichiers correspondant √† une requ√™te.
        
        Args:
            query: Requ√™te texte
            top_k: Nombre de r√©sultats √† retourner
            aggregation: M√©thode d'agr√©gation ('max', 'mean', 'weighted_mean')
        
        Returns:
            Liste des meilleurs r√©sultats avec scores de similarit√©
        """
        import os
        folder_path = Path(self.embeddings_folder)
        
        # Afficher les informations d'embedding uniquement lors de la recherche
        print(f"üìÇ Dossier embeddings utilis√©: {self.embeddings_folder}")
        print(f"üß† Mod√®le embedding: {self.embedding_model_type} ({self.embedding_model_name})")
        
        # Debug: afficher les informations
        print(f"üîç DEBUG: embeddings_folder configur√© = {self.embeddings_folder}")
        print(f"üîç DEBUG: folder_path = {folder_path}")
        print(f"üîç DEBUG: folder_path absolu = {folder_path.absolute()}")
        print(f"üîç DEBUG: folder_path existe? = {folder_path.exists()}")
        print(f"üîç DEBUG: r√©pertoire courant = {os.getcwd()}")
        
        # Lister le contenu du r√©pertoire parent
        try:
            parent = folder_path.parent
            print(f"üîç DEBUG: contenu de {parent}:")
            for item in os.listdir(parent):
                print(f"  - {item}")
        except Exception as e:
            print(f"‚ùå DEBUG: Erreur lors du listage: {e}")
        
        if not folder_path.exists():
            return {
                'success': False,
                'error': f"Le dossier d'embeddings n'existe pas: {self.embeddings_folder} (chemin absolu: {folder_path.absolute()})",
                'results': []
            }
        
        # Rechercher les fichiers .npy
        npy_files = list(folder_path.rglob("*.npy"))
        
        if len(npy_files) == 0:
            return {
                'success': False,
                'error': "Aucun fichier d'embedding trouv√© (.npy)",
                'results': []
            }
        
        # Obtenir l'embedding de la requ√™te avec le mod√®le s√©lectionn√©
        query_embedding = self.get_embedding(query)
        query_dimension = len(query_embedding)
        
        print(f"üîç DEBUG - Mod√®le embedding s√©lectionn√©: {self.embedding_model_type}")
        print(f"üîç DEBUG - Dimension embedding requ√™te: {query_dimension}")
        print(f"üîç DEBUG - Dimension attendue: {self.embedding_dim}")
        
        # V√©rifier la dimension des embeddings stock√©s (prendre le premier fichier comme r√©f√©rence)
        stored_dimension = None
        if len(npy_files) > 0:
            sample_embeddings = np.load(npy_files[0])
            if len(sample_embeddings) > 0:
                stored_dimension = len(sample_embeddings[0])
                print(f"üîç DEBUG - Dimension embeddings stock√©s: {stored_dimension}")
        
        # üÜï Si les dimensions ne correspondent pas, c'est un probl√®me critique
        # Ne PAS utiliser de fallback automatique - cela masque le probl√®me
        if stored_dimension and query_dimension != stored_dimension:
            print(f"‚ùå ERREUR CRITIQUE: Dimension incompatible!")
            print(f"   - Mod√®le s√©lectionn√©: {self.embedding_model_type} ({self.embedding_model_name})")
            print(f"   - Dimension requ√™te: {query_dimension}")
            print(f"   - Dimension stock√©e: {stored_dimension}")
            print(f"   - Dimension attendue: {self.embedding_dim}")
            print(f"‚ö†Ô∏è Le mod√®le d'embedding s√©lectionn√© ne correspond pas aux embeddings stock√©s!")
            print(f"‚ö†Ô∏è V√©rifiez que le dossier {self.embeddings_folder} contient des embeddings g√©n√©r√©s avec {self.embedding_model_name}")
            
            # Retourner une erreur explicite au lieu d'un fallback silencieux
            return {
                'success': False,
                'error': f'Dimension incompatible: le mod√®le {self.embedding_model_type} g√©n√®re des embeddings de {query_dimension} dimensions, mais les fichiers stock√©s ont {stored_dimension} dimensions. V√©rifiez que les embeddings ont √©t√© g√©n√©r√©s avec le bon mod√®le.',
                'error_type': 'dimension_mismatch',
                'query_dimension': query_dimension,
                'stored_dimension': stored_dimension,
                'embedding_model': self.embedding_model_name,
                'results': []
            }
        
        # Rechercher dans tous les fichiers
        file_results = {}
        
        for emb_file in npy_files:
            # Charger les embeddings
            embeddings = np.load(emb_file)
            
            # V√©rifier que la dimension correspond toujours
            if len(embeddings) > 0 and len(embeddings[0]) != query_dimension:
                print(f"‚ö†Ô∏è Fichier {emb_file} ignor√©: dimension {len(embeddings[0])} != {query_dimension}")
                continue
            
            # Charger les m√©tadonn√©es
            metadata_file = str(Path(emb_file).with_suffix('.json'))
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                # üÜï V√©rifier le mod√®le d'embedding utilis√© pour g√©n√©rer ces embeddings (si disponible)
                # Les fichiers peuvent avoir 'embedding_model' ou 'model' comme cl√©
                embedding_model_used = metadata.get('embedding_model') or metadata.get('model', 'unknown')
                if embedding_model_used != 'unknown':
                    # V√©rifier si le mod√®le correspond au mod√®le s√©lectionn√©
                    expected_model = self.embedding_model_name
                    if embedding_model_used != expected_model:
                        print(f"‚ö†Ô∏è ATTENTION - Fichier {Path(emb_file).name}: embeddings g√©n√©r√©s avec '{embedding_model_used}' mais mod√®le s√©lectionn√© est '{expected_model}'")
                    else:
                        print(f"‚úÖ Fichier {Path(emb_file).name}: embeddings g√©n√©r√©s avec {embedding_model_used} (correspond au mod√®le s√©lectionn√©)")
            except:
                continue
            
            # Calculer la similarit√© cosinus pour chaque chunk
            chunk_similarities = []
            best_chunk_id = 0
            best_chunk_text = ""
            best_similarity = 0
            
            for i, chunk_emb in enumerate(embeddings):
                # V√©rification suppl√©mentaire de la dimension
                if len(chunk_emb) != query_dimension:
                    continue
                    
                similarity = np.dot(query_embedding, chunk_emb) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(chunk_emb)
                )
                chunk_similarities.append(similarity)
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_chunk_id = i
                    # üÜï V√©rifier que 'chunks' existe dans les m√©tadonn√©es
                    chunks = metadata.get('chunks', [])
                    if i < len(chunks) and isinstance(chunks[i], dict):
                        best_chunk_text = chunks[i].get('text_preview', '')
                    else:
                        best_chunk_text = ''
            
            # Agr√©ger les scores par fichier
            if aggregation == 'max':
                file_score = max(chunk_similarities)
            elif aggregation == 'mean':
                file_score = np.mean(chunk_similarities)
            elif aggregation == 'weighted_mean':
                weights = np.array([1.0 / (i + 1) for i in range(len(chunk_similarities))])
                weights = weights / weights.sum()
                file_score = np.sum(np.array(chunk_similarities) * weights)
            else:
                file_score = max(chunk_similarities)
            
            # üÜï G√©rer le cas o√π 'hierarchy' n'existe pas dans les m√©tadonn√©es
            hierarchy = metadata.get('hierarchy', {})
            location = None
            
            # Essayer de r√©cup√©rer le location depuis hierarchy
            if isinstance(hierarchy, dict) and 'location' in hierarchy:
                location = hierarchy.get('location')
            
            # Si location n'est pas disponible, le construire √† partir du chemin du fichier
            if not location or location == 'N/A':
                try:
                    # Obtenir le chemin relatif du fichier JSON par rapport au dossier embeddings
                    emb_file_path = Path(emb_file)
                    embeddings_folder_path = Path(self.embeddings_folder)
                    
                    # Calculer le chemin relatif
                    try:
                        relative_path = emb_file_path.relative_to(embeddings_folder_path)
                    except ValueError:
                        # Si le fichier n'est pas dans le dossier embeddings, utiliser le nom du fichier
                        relative_path = emb_file_path.name
                    
                    # Construire le location √† partir du chemin relatif
                    # Exemple: "Anxiety_Disorders_out/SubSection1_Separation_Anxiety_Disorder.json" 
                    # -> "Anxiety_Disorders_out > SubSection1_Separation_Anxiety_Disorder"
                    path_parts = relative_path.parts[:-1]  # Exclure le nom du fichier
                    file_stem = relative_path.stem  # Nom sans extension
                    
                    if path_parts:
                        location = ' > '.join(path_parts) + ' > ' + file_stem
                    else:
                        location = file_stem
                except Exception as e:
                    # En dernier recours, utiliser le nom du fichier
                    location = Path(metadata.get('source_file', emb_file)).stem
            
            file_results[str(emb_file)] = {
                'file': metadata['source_file'],
                'file_name': Path(metadata['source_file']).name,
                'location': location,
                'similarity': float(file_score),
                'num_chunks': len(embeddings),
                'best_chunk_id': best_chunk_id,
                'best_chunk_text': best_chunk_text,
                'all_chunk_scores': [float(s) for s in chunk_similarities],
                'html_page': metadata.get('html_page', '')  # Ajouter le chemin HTML
            }
        
        # Trier par similarit√©
        results = sorted(
            file_results.values(), 
            key=lambda x: x['similarity'], 
            reverse=True
        )[:top_k]
        
        # üÜï Afficher directement les r√©sultats sans seuil minimum
        # Ajouter des informations diagnostiques
        diagnostic_info = self._generate_diagnostic_info(results) if results else {
            'suspected_pathology': None,
            'confidence': 0,
            'confidence_level': 'none',
            'message': 'Aucun r√©sultat trouv√©'
        }
        
        return {
            'success': True,
            'results': results if results else [],
            'diagnostic_info': diagnostic_info,
            'total_files_searched': len(file_results)
        }
    
    def _generate_diagnostic_info(self, results):
        """G√©n√©rer des informations diagnostiques bas√©es sur les r√©sultats."""
        if not results:
            return {
                'suspected_pathology': None,
                'confidence': 0,
                'confidence_level': 'none'
            }
        
        top_match = results[0]
        similarity_percent = top_match['similarity'] * 100
        
        pathology = top_match['file_name'].replace('.txt', '').replace('_', ' ')
        
        if similarity_percent >= 75:
            confidence_level = 'high'
            message = "Forte correspondance diagnostique"
        elif similarity_percent >= 60:
            confidence_level = 'moderate'
            message = "Correspondance mod√©r√©e - Envisager un diagnostic diff√©rentiel"
        else:
            confidence_level = 'low'
            message = "Faible confiance - Informations cliniques suppl√©mentaires n√©cessaires"
        
        return {
            'suspected_pathology': pathology,
            'confidence': similarity_percent,
            'confidence_level': confidence_level,
            'message': message
        }
    
    def generate_ai_diagnosis(self, pathology_name, form_data, similarity_score, medical_text="", historical_symptoms=None):
        """
        G√©n√©rer uniquement le plan de traitement avec OpenAI (Model 1) ou Claude (Model 2)
        bas√© sur les donn√©es du formulaire, le texte m√©dical et l'historique.
        
        Args:
            pathology_name: Nom de la pathologie valid√©e
            historical_symptoms: Liste des sympt√¥mes/crit√®res des consultations pr√©c√©dentes (optionnel)
            form_data: Donn√©es du formulaire (dict avec tous les crit√®res coch√©s)
            similarity_score: Score de similarit√© de la recherche
            medical_text: Texte m√©dical extrait du fichier source (documentation DSM-5-TR)
        
        Returns:
            dict: Plan de traitement g√©n√©r√© par l'IA
        """
        try:
            # Message syst√®me pour le PLAN DE TRAITEMENT
            system_message_treatment = (
                "Vous √™tes un psychiatre clinicien expert du DSM-5-TR. "
                "Vous r√©digez un plan de traitement d√©taill√© et structur√© en fran√ßais pour le patient. "
                "Incluez : activit√©s th√©rapeutiques (suivi th√©rapeutique), prise en charge m√©dicale si n√©cessaire, "
                "recommandations psychoth√©rapeutiques, et suivi √† long terme. "
                "Basez-vous sur les recommandations officielles (HAS, OMS, soci√©t√©s savantes). "
                "Si une information manque pour √©tablir un plan s√ªr, indiquez-le clairement."
            )
            
            # üÜï Construire le prompt pour le plan de traitement directement
            treatment_prompt = self._build_treatment_prompt(
                pathology_name, 
                form_data, 
                "",  # Pas de diagnostic text, on g√©n√®re directement le plan
                medical_text, 
                historical_symptoms
            )
            
            # üÜï G√âN√âRER UNIQUEMENT LE PLAN DE TRAITEMENT
            print("üîÑ G√©n√©ration du plan de traitement...")
            print(f"üîç DEBUG - Longueur du prompt: {len(treatment_prompt)} caract√®res")
            print(f"üîç DEBUG - Longueur du medical_text: {len(medical_text) if medical_text else 0} caract√®res")
            print(f"üîç DEBUG - Nombre de historical_symptoms: {len(historical_symptoms) if historical_symptoms else 0}")
            
            treatment_plan_text = ""
            
            # Appeler l'API selon le mod√®le s√©lectionn√©
            if self.model == 'chatgpt-5.1':
                # OpenAI / ChatGPT
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {
                            "role": "system",
                            "content": system_message_treatment
                        },
                        {
                            "role": "user",
                            "content": treatment_prompt
                        }
                    ],
                    max_completion_tokens=2000  # Limit√© pour √©viter les timeouts Heroku (30s) avec GPT-4oduit pour des rponses plus rapides (Heroku timeout 30s)
                )
                # Debug: afficher la r√©ponse compl√®te
                print(f"üîç DEBUG ChatGPT response type: {type(response)}")
                print(f"üîç DEBUG ChatGPT response.choices: {response.choices if hasattr(response, 'choices') else 'N/A'}")
                if hasattr(response, 'choices') and response.choices:
                    print(f"üîç DEBUG ChatGPT response.choices[0]: {response.choices[0]}")
                    if hasattr(response.choices[0], 'message'):
                        print(f"üîç DEBUG ChatGPT response.choices[0].message: {response.choices[0].message}")
                        if hasattr(response.choices[0].message, 'content'):
                            print(f"üîç DEBUG ChatGPT content type: {type(response.choices[0].message.content)}")
                            print(f"üîç DEBUG ChatGPT content length: {len(response.choices[0].message.content) if response.choices[0].message.content else 0}")
                
                # Extraire le contenu de la r√©ponse
                if response.choices and len(response.choices) > 0:
                    treatment_plan_text = response.choices[0].message.content
                    if not treatment_plan_text:
                        treatment_plan_text = ""
                        print(f"‚ö†Ô∏è R√©ponse ChatGPT vide - response.choices[0].message.content est None ou vide")
                        # Afficher plus de d√©tails pour le d√©bogage
                        print(f"üîç DEBUG - response.choices[0].message: {response.choices[0].message}")
                        print(f"üîç DEBUG - response.choices[0].finish_reason: {response.choices[0].finish_reason if hasattr(response.choices[0], 'finish_reason') else 'N/A'}")
                else:
                    treatment_plan_text = ""
                    print(f"‚ö†Ô∏è R√©ponse ChatGPT sans choix - response.choices est vide")
                    print(f"üîç DEBUG - response complet: {response}")
                
            elif self.model == 'claude-4.5':
                # Claude Sonnet 4.5 - utilisation directe (sans embeddings)
                try:
                    # V√©rifier que la cl√© API est configur√©e
                    if not settings.CLAUDE_API_KEY:
                        raise ValueError("CLAUDE_API_KEY n'est pas configur√© dans le fichier .env")
                    
                    print(f"üîç Appel API Claude avec mod√®le: {self.claude_model}")
                    print(f"üîç Cl√© API pr√©sente: {'Oui' if settings.CLAUDE_API_KEY else 'Non'}")
                    
                    response = self.claude_client.messages.create(
                        model=self.claude_model,  # Claude Sonnet 4.5
                        max_tokens=1200,  # R√©duit pour √©viter timeout Heroku (30s) - Claude prend ~30s avec 2000 tokens
                        system=system_message_treatment,
                        messages=[
                            {
                                "role": "user",
                                "content": treatment_prompt
                            }
                        ]
                    )
                    
                    print(f"‚úÖ R√©ponse Claude re√ßue: type={type(response)}")
                    print(f"‚úÖ Response.content: {response.content if hasattr(response, 'content') else 'N/A'}")
                    
                    # Claude retourne response.content qui est une liste de TextBlock
                    # Le premier bloc contient le texte (format: TextBlock avec attribut .text)
                    if hasattr(response, 'content') and response.content and len(response.content) > 0:
                        first_content = response.content[0]
                        
                        # Claude SDK retourne un objet TextBlock avec attribut .text
                        if hasattr(first_content, 'text'):
                            treatment_plan_text = first_content.text
                            print(f"‚úÖ Plan de traitement extrait: {len(treatment_plan_text)} caract√®res")
                        else:
                            # Fallback si format diff√©rent
                            treatment_plan_text = str(first_content)
                            print(f"‚ö†Ô∏è Format inattendu, conversion en string: {len(treatment_plan_text)} caract√®res")
                    else:
                        error_msg = f"R√©ponse Claude vide - response.content: {getattr(response, 'content', 'N/A')}"
                        print(f"‚ùå {error_msg}")
                        raise ValueError(error_msg)
                    
                    if not treatment_plan_text or len(treatment_plan_text.strip()) == 0:
                        raise ValueError("Le plan de traitement g√©n√©r√© par Claude est vide")
                    
                except Exception as claude_error:
                    # Afficher l'erreur d√©taill√©e pour le d√©bogage
                    import traceback
                    error_detail = traceback.format_exc()
                    error_msg = f"Erreur API Claude: {str(claude_error)}"
                    print(f"‚ùå {error_msg}")
                    print(f"‚ùå Mod√®le utilis√©: {self.claude_model}")
                    print(f"‚ùå Cl√© API configur√©e: {'Oui' if settings.CLAUDE_API_KEY else 'Non'}")
                    print(f"‚ùå D√©tails de l'erreur:\n{error_detail}")
                    raise RuntimeError(f"{error_msg}\n\nD√©tails: {error_detail}")
            
            else:
                raise ValueError(f"Mod√®le non support√© pour la g√©n√©ration: {self.model}")
            
            if not treatment_plan_text:
                raise ValueError("Le plan de traitement g√©n√©r√© est vide")
            
            print(f"‚úÖ Plan de traitement g√©n√©r√©: {len(treatment_plan_text)} caract√®res")
            
            return {
                'success': True,
                'pathology': pathology_name,
                'diagnosis': '',  # Pas de diagnostic summary
                'treatment_plan': treatment_plan_text,  # Uniquement le plan de traitement
                'confidence': similarity_score,
                'timestamp': self._get_timestamp(),
                'model_used': self.model
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'pathology': pathology_name,
                'model_used': self.model
            }
    
    def _build_diagnosis_prompt(self, pathology_name, form_data, similarity_score, medical_text="", historical_symptoms=None):
        """Construire le prompt pour OpenAI avec le texte m√©dical et l'historique du patient."""
        
        # Charger le fichier complet de la pathologie depuis le dossier disorders
        complete_pathology_text = self._load_complete_pathology_file(pathology_name)
        
        prompt = f"""√âlabore un R√âSUM√â DIAGNOSTIQUE (sans plan th√©rapeutique) pour un patient √©valu√© selon le DSM-5-TR.

Consignes obligatoires :
- Baser l'analyse UNIQUEMENT sur les crit√®res coch√©s ci-dessous et sur l'extrait m√©dical fourni.
- Ne jamais prescrire ni d√©crire un traitement m√©dicamenteux ou une posologie.
- Utiliser un ton clinique, structur√© et concis en fran√ßais.

Informations de r√©f√©rence :
‚Ä¢ Pathologie suspect√©e : {pathology_name}
‚Ä¢ Niveau de correspondance : {similarity_score:.1f}%

DOCUMENTATION COMPL√àTE DE LA PATHOLOGIE (DSM-5-TR) :
{complete_pathology_text if complete_pathology_text else "Documentation compl√®te non disponible."}

Extrait DSM-5-TR disponible (extrait de recherche) :
{medical_text if medical_text else "Aucun extrait suppl√©mentaire. S'appuyer uniquement sur les crit√®res coch√©s et la documentation compl√®te ci-dessus."}

Crit√®res et √©l√©ments cliniques d√©clar√©s :
"""
        
        # üÜï AJOUTER L'HISTORIQUE M√âDICAL
        if historical_symptoms and len(historical_symptoms) > 0:
            prompt += f"\nüìã **ANT√âC√âDENTS M√âDICAUX DU PATIENT ({len(historical_symptoms)} sympt√¥mes enregistr√©s):**\n"
            prompt += "Le patient pr√©sente √©galement les ant√©c√©dents cliniques suivants, issus de consultations pr√©c√©dentes:\n"
            for i, symptom in enumerate(historical_symptoms[:15], 1):  # Limiter √† 15 pour ne pas surcharger
                prompt += f"  ‚Ä¢ {symptom}\n"
            if len(historical_symptoms) > 15:
                prompt += f"  ‚Ä¢ ... et {len(historical_symptoms) - 15} autres sympt√¥mes enregistr√©s\n"
            prompt += "\n**‚ö†Ô∏è IMPORTANT : Int√©grer ces ant√©c√©dents dans l'analyse diagnostique.**\n\n"
        
        prompt += """
"""
        
        # Ajouter les donn√©es du formulaire
        if isinstance(form_data, dict):
            for key, value in form_data.items():
                if value:  # Si la valeur n'est pas vide
                    if isinstance(value, list):
                        prompt += f"\n**{key}:**\n"
                        for item in value:
                            prompt += f"  ‚úì {item}\n"
                    else:
                        prompt += f"\n**{key}:** {value}\n"
        
        prompt += """

Structure attendue (respecter EXACTEMENT ces titres) :

## 1. Synth√®se clinique
- 2 √† 3 phrases r√©sumant la pr√©sentation clinique et le niveau de confiance.

## 2. Crit√®res DSM-5 confirm√©s
- Reprendre les crit√®res coch√©s (par blocs si possible) avec le nombre total valid√©.

## 3. Diagnostic diff√©rentiel prioritaire
- 3 √† 5 hypoth√®ses maximum, chacune justifi√©e bri√®vement.

## 4. Comorbidit√©s / facteurs associ√©s
- √âl√©ments issus du formulaire ou traditionnellement li√©s √† la pathologie, avec lien clinique.

## 5. Recommandations cliniques imm√©diates
- √âtapes de suivi, examens compl√©mentaires, coordination interdisciplinaire ou psycho√©ducation.
- INTERDIT : citer des mol√©cules, dosages, ou protocoles th√©rapeutiques.
"""
        
        return prompt
    
    def _generate_treatment_plan(self, pathology_name, form_data, diagnosis_text, medical_text="", historical_symptoms=None, system_message=None):
        """
        G√©n√©rer un plan de traitement d√©taill√© pour le patient.
        
        Args:
            pathology_name: Nom de la pathologie
            form_data: Donn√©es du formulaire
            diagnosis_text: Texte du diagnostic g√©n√©r√©
            medical_text: Texte m√©dical extrait
            historical_symptoms: Historique des sympt√¥mes
            system_message: Message syst√®me pour le plan de traitement
            
        Returns:
            str: Plan de traitement g√©n√©r√©
        """
        try:
            # Construire le prompt pour le plan de traitement
            treatment_prompt = self._build_treatment_prompt(
                pathology_name, 
                form_data, 
                diagnosis_text, 
                medical_text, 
                historical_symptoms
            )
            
            # G√©n√©rer le plan de traitement avec le m√™me mod√®le
            if self.model == 'chatgpt-5.1':
                response = self.client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {
                            "role": "system",
                            "content": system_message
                        },
                        {
                            "role": "user",
                            "content": treatment_prompt
                        }
                    ],
                    max_completion_tokens=2000  # Limit√© pour √©viter les timeouts Heroku (30s) avec GPT-4oÔøΩduit pour des rÔøΩponses plus rapides (Heroku timeout 30s)
                )
                # Debug: afficher la r√©ponse compl√®te
                print(f"üîç DEBUG ChatGPT response type: {type(response)}")
                print(f"üîç DEBUG ChatGPT response.choices: {response.choices if hasattr(response, 'choices') else 'N/A'}")
                if hasattr(response, 'choices') and response.choices:
                    print(f"üîç DEBUG ChatGPT response.choices[0]: {response.choices[0]}")
                    if hasattr(response.choices[0], 'message'):
                        print(f"üîç DEBUG ChatGPT response.choices[0].message: {response.choices[0].message}")
                        if hasattr(response.choices[0].message, 'content'):
                            print(f"üîç DEBUG ChatGPT content type: {type(response.choices[0].message.content)}")
                            print(f"üîç DEBUG ChatGPT content length: {len(response.choices[0].message.content) if response.choices[0].message.content else 0}")
                
                # Extraire le contenu de la r√©ponse
                if response.choices and len(response.choices) > 0:
                    treatment_plan_text = response.choices[0].message.content
                    if not treatment_plan_text:
                        treatment_plan_text = ""
                        print(f"‚ö†Ô∏è R√©ponse ChatGPT vide - response.choices[0].message.content est None ou vide")
                        # Afficher plus de d√©tails pour le d√©bogage
                        print(f"üîç DEBUG - response.choices[0].message: {response.choices[0].message}")
                        print(f"üîç DEBUG - response.choices[0].finish_reason: {response.choices[0].finish_reason if hasattr(response.choices[0], 'finish_reason') else 'N/A'}")
                else:
                    treatment_plan_text = ""
                    print(f"‚ö†Ô∏è R√©ponse ChatGPT sans choix - response.choices est vide")
                    print(f"üîç DEBUG - response complet: {response}")
                
            elif self.model == 'claude-4.5':
                response = self.client.messages.create(
                    model=self.claude_model,
                    max_tokens=1200,  # RÔøΩduit pour des rÔøΩponses plus rapides (Heroku timeout 30s)
                    temperature=0.4,
                    system=system_message,
                    messages=[
                        {
                            "role": "user",
                            "content": treatment_prompt
                        }
                    ]
                )
                if hasattr(response, 'content') and response.content and len(response.content) > 0:
                    treatment_plan_text = response.content[0].text
                else:
                    raise ValueError("R√©ponse Claude vide pour le plan de traitement")
            else:
                raise ValueError(f"Mod√®le non support√© pour le plan de traitement: {self.model}")
            
            print(f"‚úÖ Plan de traitement g√©n√©r√©: {len(treatment_plan_text)} caract√®res")
            return treatment_plan_text
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la g√©n√©ration du plan de traitement: {str(e)}")
            return f"Erreur lors de la g√©n√©ration du plan de traitement: {str(e)}"
    
    def _build_treatment_prompt(self, pathology_name, form_data, diagnosis_text="", medical_text="", historical_symptoms=None):
        """
        Construire le prompt pour g√©n√©rer le plan de traitement.
        """
        # Charger le fichier complet de la pathologie depuis le dossier disorders
        complete_pathology_text = self._load_complete_pathology_file(pathology_name)
        
        prompt = f"""G√©n√®re un PLAN DE TRAITEMENT d√©taill√© et structur√© en fran√ßais pour un patient.

INFORMATIONS DU PATIENT :
‚Ä¢ Pathologie identifi√©e : {pathology_name}

DOCUMENTATION COMPL√àTE DE LA PATHOLOGIE (DSM-5-TR) :
{complete_pathology_text if complete_pathology_text else "Documentation compl√®te non disponible."}

TEXTE M√âDICAL DE R√âF√âRENCE (extrait de recherche) :
{medical_text[:1000] + "..." if medical_text and len(medical_text) > 1000 else (medical_text if medical_text else "Aucun extrait suppl√©mentaire.")}

CRIT√àRES VALID√âS :
"""
        
        # Ajouter les donn√©es du formulaire
        if isinstance(form_data, dict):
            for key, value in form_data.items():
                if key != '_metadata' and value:  # Exclure les m√©tadonn√©es
                    if isinstance(value, list):
                        prompt += f"\n**{key}:**\n"
                        for item in value:
                            prompt += f"  ‚úì {item}\n"
                    else:
                        prompt += f"\n**{key}:** {value}\n"
        
        # Ajouter l'historique si disponible (limiter pour √©viter les prompts trop longs - r√©duit √† 3 pour GPT-5)
        if historical_symptoms and len(historical_symptoms) > 0:
            # Limiter √† 3 sympt√¥mes les plus r√©cents pour √©viter les prompts trop longs avec GPT-5
            limited_symptoms = historical_symptoms[:3]
            prompt += f"\nüìã **ANT√âC√âDENTS M√âDICAUX (3 sympt√¥mes les plus r√©cents sur {len(historical_symptoms)}):**\n"
            for symptom in limited_symptoms:
                # Limiter la longueur de chaque sympt√¥me √† 50 caract√®res pour GPT-5
                symptom_short = symptom[:50] + "..." if len(symptom) > 50 else symptom
                prompt += f"  ‚Ä¢ {symptom_short}\n"
        
        prompt += """

STRUCTURE ATTENDUE DU PLAN DE TRAITEMENT :

## 1. Traitements M√©dicamenteux
- **OBLIGATOIRE** : Inclure TOUS les m√©dicaments recommand√©s pour cette pathologie selon la documentation DSM-5-TR
- Pour chaque m√©dicament : nom g√©n√©rique, indications, posologie recommand√©e (doses de d√©part et d'entretien)
- Dur√©e du traitement m√©dicamenteux
- Pr√©cautions et contre-indications importantes
- Interactions m√©dicamenteuses √† surveiller

## 2. Interventions Psychoth√©rapeutiques
- Type de psychoth√©rapie recommand√©e (CBT, TCC, th√©rapie d'exposition, etc.)
- Objectifs th√©rapeutiques sp√©cifiques
- Dur√©e et fr√©quence des s√©ances
- Techniques th√©rapeutiques √† utiliser

## 3. Suivi Th√©rapeutique (Activit√©s Th√©rapeutiques)
- Indiquer le type de suivi recommand√© (fr√©quence, dur√©e)
- Modalit√©s de suivi (consultations, t√©l√©consultations, etc.)
- Points de contr√¥le et √©valuations p√©riodiques

## 4. Prise en Charge M√©dicale (si n√©cessaire)
- Recommandations m√©dicales g√©n√©rales
- Suivi des comorbidit√©s physiques si pr√©sentes
- Examens compl√©mentaires n√©cessaires

## 5. Suivi √† Long Terme
- Planification du suivi sur plusieurs mois
- Points de vigilance
- Crit√®res d'am√©lioration attendus
- Strat√©gies de pr√©vention des rechutes

IMPORTANT : 
- **OBLIGATOIRE** : Base-toi sur la DOCUMENTATION COMPL√àTE DE LA PATHOLOGIE fournie ci-dessus
- **OBLIGATOIRE** : Inclure TOUS les m√©dicaments et traitements mentionn√©s dans la documentation DSM-5-TR
- Utilise un langage m√©dical professionnel
- Sois pr√©cis et d√©taill√© pour les m√©dicaments (noms, posologies, dur√©es)
- Sois pr√©cis mais adapt√© au cas du patient
- NE PAS ajouter de phrases de conclusion, de disclaimer ou de note sur l'ajustement du plan
- Terminer directement apr√®s la section 5 sans phrase de cl√¥ture
"""
        
        return prompt
    
    def _get_timestamp(self):
        """Obtenir le timestamp actuel."""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def _load_complete_pathology_file(self, pathology_name):
        """
        Charger le fichier .txt complet depuis le dossier disorders.
        
        Args:
            pathology_name: Nom de la pathologie (ex: "Agoraphobia", "Separation Anxiety Disorder")
            
        Returns:
            str: Contenu complet du fichier .txt, ou cha√Æne vide si non trouv√©
        """
        try:
            disorders_folder = settings.BASE_DIR / 'disorders'
            
            if not disorders_folder.exists():
                print(f"‚ö†Ô∏è Dossier disorders non trouv√©: {disorders_folder}")
                return ""
            
            # Nettoyer le nom de la pathologie pour la recherche
            # Convertir en format de nom de fichier (ex: "Agoraphobia" -> "SubSection*_Agoraphobia.txt")
            pathology_clean = pathology_name.strip()
            
            # Chercher dans tous les sous-dossiers
            for txt_file in disorders_folder.rglob('*.txt'):
                file_name = txt_file.stem  # Nom sans extension
                
                # V√©rifier si le nom du fichier contient le nom de la pathologie
                # ou si le nom de la pathologie correspond au d√©but du fichier
                if pathology_clean.lower() in file_name.lower() or file_name.lower().endswith(pathology_clean.lower().replace(' ', '_')):
                    # Lire le contenu complet
                    with open(txt_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    print(f"‚úÖ Fichier pathologie complet charg√©: {txt_file.name} ({len(content)} caract√®res)")
                    return content
                
                # V√©rifier aussi le contenu du fichier (premi√®re ligne contient souvent le nom)
                try:
                    with open(txt_file, 'r', encoding='utf-8') as f:
                        first_line = f.readline().strip()
                        if pathology_clean.lower() in first_line.lower():
                            # Relire tout le fichier
                            with open(txt_file, 'r', encoding='utf-8') as f2:
                                content = f2.read()
                            print(f"‚úÖ Fichier pathologie complet charg√© (par premi√®re ligne): {txt_file.name} ({len(content)} caract√®res)")
                            return content
                except:
                    continue
            
            print(f"‚ö†Ô∏è Fichier pathologie non trouv√© pour: {pathology_name}")
            return ""
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du chargement du fichier pathologie: {e}")
            import traceback
            print(traceback.format_exc())
            return ""

